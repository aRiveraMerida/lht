---
title: "La Ley de Inteligencia Artificial de la UE: Qu√© significa para tu empresa"
date: "2025-08-19"
excerpt: "Gu√≠a completa sobre la nueva Ley Europea de IA: c√≥mo te afecta, qu√© obligaciones tienes y c√≥mo prepararte desde ya para cumplirla sin sorpresas."
author: "Alberto Rivera"
category: "IA y Regulaci√≥n"
---
La nueva Ley de Inteligencia Artificial de la Uni√≥n Europea no solo afecta a quienes desarrollan IA, sino tambi√©n a cualquier empresa que **la utilice**, ya sea mediante APIs, herramientas como Copilot o sistemas integrados en sus procesos. Esta ley establece obligaciones **seg√∫n el riesgo del uso que se haga de la IA**, no seg√∫n qui√©n la cre√≥.

Si usas IA para tareas sensibles ‚Äîcomo evaluar candidatos, conceder cr√©ditos o automatizar decisiones‚Äî entras en categor√≠a de **riesgo alto** y debes cumplir medidas estrictas (supervisi√≥n humana, documentaci√≥n, transparencia, monitoreo). Si solo usas IA para generar contenido o atenci√≥n al cliente, hablamos de **riesgo limitado**, donde la **transparencia es obligatoria**. Y si la usas para tareas internas sin impacto directo, como redactar borradores o clasificar correos, probablemente est√©s en **riesgo m√≠nimo**, pero **igual debes actuar con responsabilidad**.

La Ley se aplicar√° por fases a partir de agosto de 2024. Las empresas que se preparen desde ya ‚Äîhaciendo inventario, clasificando usos, formando equipos y documentando decisiones‚Äî no solo evitar√°n sanciones, sino que estar√°n mejor posicionadas en un mercado que exige **IA √©tica, segura y fiable**.

## **¬øQu√© es la Ley de Inteligencia Artificial de la UE y por qu√© deber√≠a importarte?**

---

Este documento no es solo otro ‚Äúchecklist legal‚Äù. Es una gu√≠a pensada para ayudarte a entender, de forma pr√°ctica, qu√© significa para tu empresa la nueva **Ley Europea de Inteligencia Artificial (Reglamento 2024/1689)**, c√≥mo te afecta (aunque no desarrolles IA), y qu√© pasos concretos debes empezar a dar ya.

La UE ha aprobado la **primera gran ley global sobre IA**, y su enfoque es claro: quiere que la IA que se use en Europa sea **segura, √©tica, transparente y respetuosa con los derechos fundamentales**. Nada de cajas negras que decidan sobre personas sin supervisi√≥n. Esta ley pone el foco en proteger a las personas, dar seguridad jur√≠dica a las empresas y fomentar una IA de confianza, especialmente para que las pymes puedan innovar sin miedo.

Lo m√°s importante: esta ley **no trata a toda la IA por igual**, sino que regula seg√∫n el nivel de riesgo que implica su uso. Clasifica los sistemas de IA en cuatro niveles:

üî¥ **Riesgo Inaceptable** (directamente prohibido).

üü† **Riesgo Alto** (regulado con requisitos estrictos).

üü° **Riesgo Limitado** (con reglas de transparencia).

üü¢ **Riesgo M√≠nimo** (sin obligaciones legales, pero con buenas pr√°cticas).

Y s√≠, **esto tambi√©n aplica si tu empresa solo usa herramientas como Microsoft Copilot, ChatGPT o una API de IA**. No importa tanto qu√© IAs usas, sino **para qu√© la est√°s usando**.

Adem√°s, no se limita a empresas dentro de la UE: si vendes, despliegas o haces llegar resultados de IA al mercado europeo, aunque est√©s fuera, esta ley te puede aplicar igual. Las multas por incumplir no son menores: hasta **35 millones de euros o el 7% de tu facturaci√≥n global**, lo que sea m√°s alto.

Pero no todo es amenaza. Cumplir esta ley puede ser una **ventaja competitiva**, igual que lo fue en su momento el cumplimiento del RGPD. Adoptarla bien desde el principio te permite construir confianza, anticiparse a regulaciones similares fuera de Europa y evitar redise√±os forzados a √∫ltima hora. Por eso, **esperar al √∫ltimo minuto no es una opci√≥n**.

Perfecto, aqu√≠ tienes la reescritura del segundo bloque con un tono m√°s claro, directo y enfocado a empresas reales que podr√≠an estar usando IA sin ser del todo conscientes del alcance de la Ley:

## **¬øAplica la Ley de IA a tu empresa? Lo primero que tienes que verificar**

---

Antes de meterte en los detalles t√©cnicos o legales, necesitas responder una pregunta clave:

| **Pregunta:** | **S√≠** | **No** |
| --- | --- | --- |
| ¬øComercializa tu empresa sistemas de IA en la Uni√≥n Europea? |  |  |
| ¬øEsta ley aplica a lo que hace tu empresa con IA? |  |  |
| ¬øImplementa tu empresa sistemas de IA en la UE? |  |  |
| ¬øUtilizas sistemas de IA en la UE aunque no los hayas desarrollado? |  |  |
| ¬øSe utilizan en Europa los resultados de tus sistemas de IA, a pesar de estar ubicado fuera de la UE? |  |  |
| ¬øImportas o distribuyes sistemas de IA en el mercado europeo? |  |  |
| ¬øEres el representante legal en la UE de una empresa no europea que vende IA? |  |  |
| ¬øTu empresa introduce sistemas de IA en el mercado de la UE? |  |  |
| ¬øTu empresa pone en funcionamiento (despliega) sistemas de IA en la UE? |  |  |
| ¬øEres un usuario de IA (aunque no la hayas creado) ubicado dentro de la UE? |  |  |
| ¬øEst√°s fuera de la UE, pero el resultado de tu sistema de IA se utiliza dentro de Europa? |  |  |
| ¬øImportas o distribuyes sistemas de IA en el mercado europeo? |  |  |
| ¬øEres representante legal de una empresa no europea que vende IA en la UE? |  |  |

### **¬øQu√© se considera un ‚Äúsistema de IA‚Äù?**

La definici√≥n legal es m√°s amplia de lo que parece. Seg√∫n la ley, es IA cualquier sistema **basado en m√°quinas** que:

- Funcione de forma **aut√≥noma** (aunque sea parcial),
- Tenga **cierta capacidad de adaptarse** despu√©s de estar en uso,
- E infiera, a partir de los datos que recibe, **c√≥mo generar resultados** como recomendaciones, decisiones, contenido, etc.,
- Que puedan influir en **entornos f√≠sicos o virtuales** (por ejemplo, personas, sistemas de decisi√≥n, procesos de negocio, etc.).

Esto incluye mucho m√°s que los modelos grandes de IA. Un sistema de scoring, un motor de recomendaciones, un algoritmo que decide si un cliente califica o no‚Ä¶ todo esto **puede entrar en la definici√≥n de sistema de IA.**

> Se excluyen herramientas muy b√°sicas basadas en reglas fijas (tipo ‚Äúsi A, entonces B‚Äù) creadas por programadores sin inferencia ni autonom√≠a. Pero **casi todo lo que usamos hoy como ‚ÄúIA‚Äù entra en esta definici√≥n**
> 

### **¬øQu√© rol tiene tu empresa seg√∫n la ley?**

Tu nivel de obligaciones depender√° del rol que juegas con la IA:

| **Rol** | **¬øQu√© significa?** |
| --- | --- |
| Proveedor | Creas, desarrollas o integras el sistema de IA y lo introduces en el mercado. |
| Desplegador (usuario) | Eres quien lo usa para tomar decisiones, prestar servicios, o generar contenido. |
| Importador | Traes el sistema de fuera y lo vendes dentro de la UE. |
| Distribuidor | No lo desarrollas t√∫, pero lo comercializas. |
| Representante Autorizado | Eres la persona o empresa que representa a un proveedor fuera de la UE. |

> Si modificas un sistema de IA (por ejemplo, le cambias el prop√≥sito, haces ajustes relevantes, o le pones tu marca), **puedes pasar a tener las mismas obligaciones que un proveedor**. Esto no es trivial: te puede cambiar totalmente la carga legal y t√©cnica que asumes.
> 

### **¬øHay casos en los que est√°s excluido?**

S√≠, la ley no aplica a todo. Estas son las principales **exclusiones**:

- Sistemas de IA usados exclusivamente para **fines militares o de defensa nacional**.
- Sistemas de IA usados solo para **investigaci√≥n y desarrollo cient√≠fico**, aunque si los pruebas en entornos reales, s√≠ pueden tener requisitos.
- Algunos **modelos de c√≥digo abierto y gratuito**, aunque tambi√©n aqu√≠ hay matices si los integras en productos de riesgo.

### **Lo que no deber√≠as subestimar**

Esta ley fue dise√±ada con una l√≥gica muy amplia: es **tecnol√≥gicamente neutral** y **preparada para el futuro**. Esto significa que regula el uso, sin importar si la IA se basa en machine learning, reglas, modelos fundacionales, etc.

Y lo m√°s importante: **tambi√©n aplica aunque no vendas en la UE directamente.** Basta con que el resultado de tu IA se use aqu√≠ (por ejemplo, un chatbot entrenado fuera pero operando para clientes europeos) para que est√©s dentro del alcance.

Adem√°s, tu rol puede cambiar con el tiempo. Hoy puedes ser un simple usuario, pero si ma√±ana modificas un sistema o integras una API de forma personalizada, podr√≠as asumir nuevas responsabilidades. Por eso, **evaluar tu exposici√≥n a esta ley no es una tarea √∫nica: debe revisarse de forma continua**.

Perfecto, aqu√≠ tienes la reescritura del tercer bloque, manteniendo toda la profundidad t√©cnica pero explicada con un enfoque pr√°ctico, claro y empresarial:

## **Paso clave: ¬øQu√© nivel de riesgo tienen tus sistemas de IA?**

---

Una vez que sabes que la Ley de IA te aplica, el siguiente paso es **clasificar correctamente cada uso de IA en tu empresa seg√∫n su nivel de riesgo**. Esta clasificaci√≥n determinar√° **qu√© obligaciones legales tendr√°s que cumplir** (y algunas son bastante exigentes). La Ley distingue **cuatro niveles de riesgo**:

| **Nivel de Riesgo** | **¬øQu√© significa?** | **Ejemplos comunes** | **Consecuencias principales** |
| --- | --- | --- | --- |
| üî¥ **Inaceptable** | Est√° prohibido usarlo, punto. | Puntuaci√≥n social, manipulaci√≥n subliminal, explotaci√≥n de menores | No puedes usar este sistema. |
| üü† **Alto** | Impacta en derechos/personas. | Reclutamiento, cr√©ditos, diagn√≥sticos m√©dicos, acceso a educaci√≥n | Cumplimiento estricto y muchas obligaciones. |
| üü° **Limitado** | Puede confundir si no se informa. | Chatbots, deepfakes, IA que detecta emociones | Reglas de transparencia y avisos obligatorios. |
| üü¢ **M√≠nimo** | Uso general, sin riesgo significativo. | Filtros de spam, IA en videojuegos, Copilot para productividad | No hay obligaciones legales espec√≠ficas. |

## **¬øC√≥mo saber en qu√© nivel est√°s?**

---

### **Paso 1: ¬øTu sistema entra en las pr√°cticas prohibidas?**

La Ley **proh√≠be ciertos usos de la IA directamente**. Si haces alguna de estas cosas, debes dejar de hacerlo antes de febrero de 2025:

- Manipulaci√≥n subliminal que cause da√±o.
- Explotaci√≥n de vulnerabilidades (ni√±os, personas con discapacidad, etc.).
- Puntuaci√≥n social (como en China).
- Identificaci√≥n biom√©trica en tiempo real sin control estricto (por ejemplo, reconocimiento facial en espacios p√∫blicos).
- An√°lisis emocional obligatorio en trabajo o educaci√≥n.

> Si tu IA hace alguna de estas cosas ‚Üí **no puedes usarla en Europa**.
> 

### **Paso 2: ¬øEst√°s en la categor√≠a de riesgo alto?**

Aqu√≠ entran los sistemas que:

1. **Son parte de productos regulados** (como coches, maquinaria m√©dica o juguetes), y adem√°s usan IA para seguridad ‚Üí Necesitan una **evaluaci√≥n de conformidad** (como con el marcado CE).
2. **Se usan en sectores cr√≠ticos** (Anexo III), por ejemplo:
    - Contrataci√≥n o evaluaci√≥n de trabajadores.
    - Acceso a educaci√≥n, sanidad o vivienda.
    - Concesi√≥n de cr√©ditos o seguros.
    - Justicia, migraci√≥n o control fronterizo.
    - Infraestructuras cr√≠ticas como transporte, energ√≠a o agua.

> Si tu IA se usa en uno de estos contextos, se considera **de alto riesgo (HRAIS)**, con obligaciones muy estrictas.
> 

### **Paso 3: ¬øPuedes aplicar una excepci√≥n (derogaci√≥n)?**

Si tu caso cae en el Anexo III pero **el uso concreto de tu IA no representa un riesgo significativo**, podr√≠as pedir una excepci√≥n‚Ä¶ pero cuidado:

Solo puedes hacerlo si:

- Tu sistema **no reemplaza la evaluaci√≥n humana**.
- Solo realiza tareas preparatorias o de apoyo.
- Mejora procesos humanos, sin automatizar decisiones sensibles.
- No genera **perfiles personales** ‚Üí *la elaboraci√≥n de perfiles siempre ser√° considerada de alto riesgo*.

**¬øAplica esta excepci√≥n?**

- Tienes que documentar todo antes de usar el sistema.
- Igual tienes que **registrarlo en la base de datos oficial**.
- Est√°s sujeto a revisi√≥n por autoridades.

> Esta derogaci√≥n es √∫til, pero **no es una v√≠a r√°pida para saltarte obligaciones**: requiere pruebas s√≥lidas, documentaci√≥n t√©cnica y auditor√≠a posible.
> 

### **Paso 4: ¬øEs de riesgo limitado?**

Si no est√°s en los casos anteriores, tu sistema podr√≠a estar en **riesgo limitado**. Aqu√≠ entran las IA que interact√∫an con personas y podr√≠an causar **confusi√≥n si no se explica que son IA**. Ejemplos:

- Chatbots en atenci√≥n al cliente.
- Generadores de contenido tipo deepfake.
- Sistemas que detectan emociones o hacen categorizaciones biom√©tricas **no sensibles**.

> En estos casos, debes cumplir con **requisitos de transparencia**: decir claramente que se est√° interactuando con una IA o que el contenido fue generado artificialmente.
> 

### **Paso 5: ¬øEs de riesgo m√≠nimo?**

La mayor√≠a de los usos empresariales m√°s comunes **entran aqu√≠**. No presentan un riesgo significativo ni requieren obligaciones legales espec√≠ficas.

Ejemplos t√≠picos:

- Sistemas de IA en videojuegos.
- Filtros de spam.
- Copilot para escribir emails, resumir documentos o generar ideas.
- Recomendadores simples (por ejemplo, productos relacionados).

> Aunque no hay obligaciones, se recomienda seguir buenas pr√°cticas (documentar su uso, formar al equipo, revisar los resultados antes de tomar decisiones con ellos, etc.).
> 

### **Importante: clasificar como ‚Äúalto riesgo‚Äù - no es autom√°tico**

Solo estar en una categor√≠a del Anexo III **no basta**. Tienes que evaluar si realmente existe un **riesgo significativo** para la salud, la seguridad o los derechos fundamentales. La ley te deja demostrar (con documentaci√≥n s√≥lida) que tu sistema **no** representa ese riesgo‚Ä¶ pero si te equivocas, las sanciones pueden ser serias.

Adem√°s, aunque obtengas una derogaci√≥n, **sigues teniendo que registrar el sistema**, y las autoridades podr√°n cuestionar tu evaluaci√≥n si no est√° bien fundamentada.

## **Checklist de Riesgo Inaceptable: Pr√°cticas de IA que estar√°n prohibidas en la UE desde febrero de 2025**

---

Este punto **no es negociable**: hay ciertos usos de la inteligencia artificial que estar√°n **completamente prohibidos en toda la Uni√≥n Europea** a partir del 2 de febrero de 2025. No importa si eres desarrollador, distribuidor o usuario. Si haces alguna de estas cosas con IA, tienes que parar.

### **¬øQu√© pr√°cticas est√°n prohibidas?**

Aqu√≠ tienes la lista completa de **lo que no se puede hacer** con IA bajo ning√∫n concepto:

| **Pr√°ctica Prohibida** | **S√≠** | **No** |
| --- | --- | --- |
| Usar t√©cnicas subliminales, manipuladoras o enga√±osas que distorsionen el comportamiento causando da√±o significativo |  |  |
| Explotar vulnerabilidades (edad, discapacidad, pobreza) para manipular a personas vulnerables causando da√±o |  |  |
| Aplicar sistemas de puntuaci√≥n social (tipo ‚Äúcr√©dito social‚Äù) por parte de autoridades p√∫blicas |  |  |
| Evaluar la probabilidad de que una persona cometa un delito **solo** con perfiles o caracter√≠sticas personales |  |  |
| Crear o ampliar bases de datos de reconocimiento facial mediante scraping masivo (de internet, c√°maras, etc.) |  |  |
| Detectar emociones en el lugar de trabajo o en la escuela (excepto por razones m√©dicas o de seguridad) |  |  |
| Clasificar personas por atributos sensibles (raza, ideolog√≠a, religi√≥n, orientaci√≥n sexual, etc.) salvo casos muy espec√≠ficos de aplicaci√≥n de la ley |  |  |
| Usar reconocimiento biom√©trico remoto en tiempo real en espacios p√∫blicos para la polic√≠a (excepto con autorizaci√≥n y solo en delitos muy graves) |  |  |

## **¬øC√≥mo saber si tu empresa est√° en riesgo de incumplir?**

---

Hazte estas preguntas:

- ¬øEst√°s desarrollando, vendiendo o usando alg√∫n sistema de IA que haga una o m√°s de estas cosas?
- ¬øTienes modelos de IA que podr√≠an terminar siendo usados as√≠ por tus clientes o usuarios finales?
- ¬øTu cartera de productos, roadmap o pipeline de IA incluye alguna funcionalidad que se acerque a estas pr√°cticas?

Si la respuesta es ‚Äús√≠‚Äù o ‚Äúno estoy seguro‚Äù, necesitas revisar urgentemente esos casos. Y si est√°s usando tecnolog√≠as de terceros, debes **auditar sus capacidades** y asegurarte de que no est√°s incurriendo en un uso prohibido sin darte cuenta.

### **¬øY si estas pr√°cticas son parte de un sistema mayor?**

No importa. **Aunque solo sea una parte del sistema**, si realiza alguna de estas funciones prohibidas, **tienes que eliminarla o desactivarla completamente**. No hay excepciones por ‚Äúbajo impacto‚Äù, ‚Äúuso limitado‚Äù o ‚Äúfines comerciales‚Äù.

### **¬øHay excepciones?**

S√≠, pero **muy limitadas** y casi exclusivamente para:

- **Fines m√©dicos**, cuando el an√°lisis emocional o biom√©trico tenga un prop√≥sito terap√©utico.
- **Aplicaci√≥n de la ley**, pero solo con **autorizaci√≥n judicial previa**, para **delitos muy graves**, y bajo condiciones estrictas.

> Si no est√°s en uno de esos supuestos regulados expresamente, no puedes usar estas tecnolog√≠as. Y aunque lo est√©s, tendr√°s que cumplir requisitos legales muy exigentes.
> 

### **Buenas pr√°cticas para protegerte desde ya**

1. **Revisa todos tus sistemas de IA actuales y en desarrollo.**
    - ¬øTienen alguna de estas funcionalidades, aunque sea parcial?
    - ¬øC√≥mo se entrenaron? ¬øQu√© datos usan?
2. **Haz limpieza preventiva.**
    - Si hay algo que se parezca a una de estas pr√°cticas, considera eliminarlo cuanto antes.
3. **Define una pol√≠tica interna clara.**
    - Proh√≠be expresamente el desarrollo, integraci√≥n o uso de IA que entre en estas categor√≠as.
4. **Habla con tus proveedores.**
    - Pregunta si sus herramientas incluyen alguna de estas capacidades (por defecto o habilitables).
    - Revisa contratos, documentaci√≥n t√©cnica y pol√≠ticas de uso.

### **¬øPor qu√© es tan importante?**

Estas pr√°cticas est√°n **radicalmente en contra de los valores de la UE**: dignidad humana, protecci√≥n de menores, privacidad, no discriminaci√≥n. No hay margen para la interpretaci√≥n comercial aqu√≠. Incluso si alguna de estas funciones te parece √∫til o te la han pedido clientes, **no podr√°s usarla legalmente en Europa.**

La intenci√≥n de la UE es clara: **evitar que la IA se convierta en una herramienta de control, manipulaci√≥n o discriminaci√≥n masiva**.

## **Checklist 3: ¬øTu IA es de Alto Riesgo? Esto es lo que debes cumplir (s√≠ o s√≠)**

---

La **IA de Alto Riesgo (HRAIS)** est√° en el centro de la regulaci√≥n. No se proh√≠be, pero s√≠ se regula de forma muy estricta porque puede **afectar directamente a la vida de las personas**: su salud, sus derechos, su empleo, su acceso a servicios clave.

Y si usas este tipo de IA, seas proveedor o usuario, **tienes mucho que demostrar**: que tu sistema es fiable, seguro, explicable, controlable y justo.

### **¬øQu√© se considera IA de alto riesgo?**

Seg√∫n el Anexo III, se incluyen usos de IA en estos √°mbitos:

| **√Åmbito** | **Ejemplos comunes** |
| --- | --- |
| **Biometr√≠a (no prohibida)** | Identificaci√≥n remota, reconocimiento de emociones, categorizaci√≥n por atributos |
| **Infraestructuras cr√≠ticas** | IA en tr√°fico, agua, electricidad, redes inteligentes |
| **Educaci√≥n y formaci√≥n** | Evaluaci√≥n de ex√°menes, acceso a estudios, seguimiento de estudiantes |
| **Empleo y RR.HH.** | Reclutamiento, asignaci√≥n de tareas, evaluaci√≥n de rendimiento |
| **Servicios esenciales** | Cr√©dito, seguros, ayudas p√∫blicas |
| **Justicia y polic√≠a** | An√°lisis de pruebas, predicci√≥n de reincidencia, evaluaci√≥n de riesgos |
| **Migraci√≥n y asilo** | Evaluaci√≥n de solicitudes, an√°lisis de riesgos |
| **Procesos democr√°ticos** | Influencia en elecciones, investigaci√≥n jur√≠dica automatizada |

> Si tu IA entra en alguno de estos casos, entra en la categor√≠a de **Alto Riesgo** y debes cumplir con una serie de obligaciones muy claras. Vamos a verlas.
> 

### **Obligaciones para Proveedores de HRAIS (quien crea o integra el sistema)**

Estas obligaciones aplican si t√∫:

- Desarrollas el sistema de IA.
- Lo integras en un producto que vendes.
- Lo modificas de forma relevante (y por tanto pasas a ser proveedor legal).

**Lo que tienes que implementar como proveedor de IA de alto riesgo:**

| **Requisito** | **¬øEst√°s cumpliendo?** | **¬øQu√© implica?** |
| --- | --- | --- |
| **SGC - Sistema de Gesti√≥n de Calidad** |  | Documentar todo el proceso: dise√±o, control, pruebas, recursos, monitoreo post-venta. |
| **Gesti√≥n de Riesgos** |  | Evaluar y mitigar riesgos durante todo el ciclo de vida. Incluir usos indebidos previsibles. |
| **Gobernanza de Datos** |  | Asegurar que los datos est√°n bien seleccionados, documentados y sin sesgos inaceptables. |
| **Documentaci√≥n T√©cnica** |  | Instrucciones completas del sistema, l√≥gica, limitaciones, datos usados, riesgos, mantenimiento. |
| **Registros (logs)** |  | El sistema debe registrar eventos clave y t√∫ debes guardar esos logs. |
| **Transparencia** |  | Proveer instrucciones claras a quien usar√° la IA. |
| **Supervisi√≥n Humana** |  | Asegurar que se puede intervenir, entender y corregir. |
| **Exactitud y Robustez** |  | Definir m√©tricas de precisi√≥n, robustez ante errores y defensa frente a ciberataques. |
| **Evaluaci√≥n de Conformidad** |  | Validaci√≥n t√©cnica obligatoria. Puede requerir revisi√≥n por terceros notificados. |
| **Marcado CE y Declaraci√≥n de Conformidad** |  | El sistema debe tener marcado CE antes de lanzarse al mercado. |
| **Registro en la Base de Datos de la UE** |  | Antes de lanzar, debes registrar el sistema. Tambi√©n mantener actualizada esa informaci√≥n. |
| **Monitoreo Post-Comercializaci√≥n** |  | Crear un sistema de seguimiento del rendimiento y de notificaci√≥n de incidentes. |
| **Acciones Correctivas** |  | Debes poder informar y reaccionar ante fallos graves: retirada, actualizaci√≥n, etc. |
| **Representante en la UE (si est√°s fuera)** |  | Si no tienes sede en la UE, necesitas un representante legal dentro de ella. |

Todos estos puntos est√°n **interconectados**. Por ejemplo: si tienes mala gobernanza de datos, probablemente falles tambi√©n en precisi√≥n, gesti√≥n de riesgos y cumplimiento legal.

> Necesitas una **estrategia de cumplimiento integral**, no documentos sueltos ni auditor√≠as parciales.
> 

### **Obligaciones para Usuarios o Desplegadores de HRAIS**

Aunque no desarrolles IA, si la usas para tomar decisiones en los √°mbitos anteriores, tienes tus propias responsabilidades. **Como empresa que usa IA de alto riesgo, debes:**

| **Obligaci√≥n** | **¬øCumples?** | **¬øQu√© hacer?** |
| --- | --- | --- |
| **Usar la IA seg√∫n las instrucciones del proveedor** |  | No puedes usar la IA para algo que no est√° previsto o validado. |
| **Asignar supervisi√≥n humana competente** |  | Una persona formada y autorizada debe poder intervenir si algo va mal. |
| **Validar los datos de entrada** |  | Si introduces datos en el sistema, deben ser correctos y representativos. |
| **Supervisar el funcionamiento del sistema** |  | No es plug & play: hay que monitorear el uso real. |
| **Guardar registros (logs)** |  | Conservar logs al menos 6 meses (salvo excepciones legales). |
| **Reportar incidentes y suspender uso si hay riesgo** |  | Si el sistema falla o representa un riesgo, debes detener su uso y avisar. |
| **FRIA ‚Äì Evaluaci√≥n de Impacto en Derechos** |  | Obligatoria en ciertos casos (ver abajo). |
| **Registro en la base de datos (solo sector p√∫blico)** |  | Solo aplica a entidades p√∫blicas o que act√∫an en su nombre. |
| **Informar a empleados si se usa IA en su contexto** |  | Necesitas avisar de forma clara si la IA afecta al personal. |

> Los usuarios no pueden ser pasivos: son responsables de **vigilar, documentar y actuar** si la IA genera consecuencias imprevistas.
> 

### **¬øCu√°ndo debes hacer una Evaluaci√≥n de Impacto en Derechos Fundamentales (FRIA)?**

Esta evaluaci√≥n sirve para medir **c√≥mo puede afectar el sistema a la dignidad, libertad o igualdad de las personas**, m√°s all√° de la protecci√≥n de datos (aunque puede integrarse con una DPIA del RGPD). **Aplica la FRIA si:**

- Eres una **entidad p√∫blica o privada que presta servicios p√∫blicos**, y usas HRAIS.
- Usas IA en procesos de:
    - Concesi√≥n de cr√©ditos o evaluaci√≥n de solvencia.
    - Fijaci√≥n de precios o riesgos en seguros de vida/salud.

Debes realizar la FRIA **antes del primer uso** del sistema, y actualizarla si cambian elementos clave. **¬øQu√© debe incluir?**

| **Elemento** | **¬øIncluido?** |
| --- | --- |
| Descripci√≥n del uso previsto de la IA |  |
| Per√≠odo y frecuencia de uso |  |
| Personas/grupos afectados |  |
| Riesgos espec√≠ficos identificados |  |
| C√≥mo se implementa la supervisi√≥n humana |  |
| Medidas en caso de que los riesgos se materialicen |  |

Adem√°s:

- Puedes integrar la FRIA con la **Evaluaci√≥n de Protecci√≥n de Datos (DPIA)** si ya est√°s obligado por RGPD.
- Debes estar preparado para **notificar los resultados** a la autoridad reguladora (la Oficina de IA) cuando sea requerido.

Trabajar con IA de alto riesgo **no es ilegal, pero s√≠ es una gran responsabilidad**. El cumplimiento exige una visi√≥n de ciclo de vida: desde el dise√±o hasta el monitoreo continuo tras el despliegue. No basta con decir ‚Äúmi IA funciona‚Äù; hay que poder demostrar que:

- Es segura.
- Est√° bien entrenada.
- Se usa dentro de sus l√≠mites.
- Puede ser supervisada.
- Y si algo falla, se puede corregir o detener.

## **Checklist 4: ¬øTu sistema usa IA de forma visible? Aseg√∫rate de cumplir las reglas de transparencia**

---

Aunque tu IA **no sea de alto riesgo**, si interact√∫a con personas o genera contenido, igual **tienes obligaciones legales**. ¬øPor qu√©? Porque la ley quiere evitar que la gente **sea enga√±ada o manipulada** sin saber que est√° hablando con una IA o consumiendo contenido generado artificialmente. Esta es la esencia del **Art√≠culo 50** de la Ley de IA de la UE.

### **¬øQu√© sistemas entran en esta categor√≠a?**

- Chatbots de atenci√≥n al cliente.
- Asistentes virtuales o formularios autom√°ticos.
- Avatares generados por IA para redes sociales.
- Generadores de im√°genes, audio o video (como deepfakes).
- Herramientas que crean noticias, textos o res√∫menes sin intervenci√≥n humana.
- Sistemas que detectan emociones o clasifican rostros por g√©nero, edad o estado de √°nimo (pero sin funciones de seguridad o vigilancia).

### **¬øQu√© debes hacer como empresa para cumplir?**

La ley exige **transparencia clara, comprensible y accesible** para que las personas sepan cu√°ndo est√°n interactuando con IA o consumiendo contenido generado artificialmente. Aqu√≠ tienes el checklist que debes aplicar:

| **Requisito** | **¬øCumples?** |
| --- | --- |
| **Interacci√≥n con humanos (chatbots, asistentes)**: ¬øInformas claramente a las personas de que est√°n hablando con una IA (salvo que sea obvio)? |  |
| **Sistemas que detectan emociones o rasgos biom√©tricos (no prohibidos)**: ¬øInformas a los usuarios que est√°n siendo analizados? |  |
| **Deepfakes (imagen, audio o video manipulado por IA)**: ¬øEst√°n claramente marcados como generados por IA en un formato legible por m√°quina? |  |
| **Texto generado por IA que informa al p√∫blico** (noticias, informes, etc.): ¬øSe indica que es contenido generado por IA (a menos que lo revise un humano)? |  |
| **Claridad y accesibilidad**: ¬øEsa informaci√≥n es visible, entendible y est√° presente desde la primera interacci√≥n? |  |

### **Ejemplos aplicados**

Si tu web usa un chatbot, debe mostrar un mensaje como:

> ‚ÄúHola, soy un asistente autom√°tico. ¬øEn qu√© puedo ayudarte?‚Äù
> 

Si usas IA para generar im√°genes o videos realistas (deepfakes) para campa√±as:

> Deben incluir marcas visibles que digan ‚ÄúContenido generado por IA‚Äù y estar **etiquetados t√©cnicamente**
> 

Si publicas art√≠culos generados con IA en una newsletter o blog:

> Debes informar si el texto fue generado por IA, **a menos que haya pasado por revisi√≥n humana con control editorial**
> 

Si usas reconocimiento emocional en una encuesta o experiencia digital:

> Tienes que explicar al usuario que se est√°n analizando sus expresiones o reacciones mediante IA.
> 

### **¬øHay excepciones?**

S√≠, pero limitadas:

- Obras art√≠sticas o de parodia no necesitan avisos expl√≠citos.
- Casos de aplicaci√≥n de la ley, cuando la transparencia comprometa investigaciones, tambi√©n pueden quedar exentos (muy regulado).

> **Pero cuidado**: si no est√°s claramente en estas categor√≠as, **la transparencia es obligatoria.**
> 

### **¬øPor qu√© esto importa tanto?**

Porque aunque estos sistemas no sean de ‚Äúalto riesgo‚Äù, **pueden influir en la percepci√≥n, las decisiones y la confianza de las personas**. Y la UE quiere garantizar que:

- Nadie hable con una IA pensando que es una persona.
- Nadie consuma un contenido falso sin saber que fue manipulado o generado autom√°ticamente.
- Nadie sea analizado biom√©tricamente sin su conocimiento.

Adem√°s, el **marcado t√©cnico legible por m√°quina** (en deepfakes, por ejemplo) anticipa un futuro en el que navegadores, plataformas y sistemas puedan **detectar autom√°ticamente contenido generado por IA**, no solo a trav√©s de una etiqueta visible.

Aunque tu IA no haga nada ‚Äúpeligroso‚Äù, si interact√∫a con personas o genera contenido visual/textual, **debes ser transparente**. Este es un **requisito legal y √©tico** que adem√°s te ayuda a construir confianza con tus usuarios.

No lo dejes para despu√©s: revisa desde ya tus webs, apps, contenidos y flujos conversacionales, y a√±ade los avisos y etiquetas necesarios.

## **Checklist 5: ¬øTu modelo es de prop√≥sito general (GPAI)? Entonces tienes nuevas obligaciones, aunque no controles su uso final**

---

La Ley de IA de la UE incorpora por primera vez regulaciones **directas sobre los desarrolladores de modelos base**. Es decir, **no solo regula c√≥mo se usa la IA, sino c√≥mo se construye**, sobre todo cuando hablamos de modelos muy potentes que luego otros reutilizan o integran: los llamados **Modelos de Prop√≥sito General de IA (GPAI)**.

### **¬øQu√© es un modelo GPAI?**

Es un modelo de IA que:

- **No est√° dise√±ado para una √∫nica tarea concreta**, sino que puede aplicarse a m√∫ltiples fines.
- Tiene una **generalidad significativa**.
- Puede **ejecutar distintas tareas con cierto nivel de competencia** (por ejemplo, generar texto, im√°genes, c√≥digo, audio, etc.).

üîç Ejemplos comunes: GPT-4, LLaMA, Claude, Gemini, Stable Diffusion, Mistral, etc.

**Si desarrollas, entrenas o publicas este tipo de modelo, la Ley te considera proveedor de GPAI y te impone obligaciones espec√≠ficas.**

> Estas reglas entran en vigor el **2 de agosto de 2025**
> 

### **Obligaciones para todos los proveedores de GPAI**

Si tu modelo cumple con la definici√≥n de GPAI, tienes que cumplir con los requisitos b√°sicos del Art√≠culo 53:

| **Obligaci√≥n** | **¬øCumples?** |
| --- | --- |
| **Documentaci√≥n T√©cnica completa y actualizada:** Explica c√≥mo se entren√≥, qu√© datos se usaron, c√≥mo se prob√≥, etc. |  |
| **Proporcionar documentaci√≥n a quienes usen tu modelo aguas abajo:** Incluye capacidades, limitaciones, instrucciones para uso seguro. |  |
| **Respetar la ley de Copyright de la UE:** Debes declarar si aplicas la ‚Äúreserva de derechos‚Äù bajo la excepci√≥n TDM (Text & Data Mining). |  |
| **Publicar un resumen detallado del contenido usado para entrenar el modelo** |  |

> Estos requisitos aplican **aunque tu modelo no se use directamente en un sistema de alto riesgo**.
> 

### **Obligaciones adicionales si tu modelo tiene Riesgo Sist√©mico**

La Ley reconoce que algunos modelos son tan grandes o poderosos que pueden generar riesgos en s√≠ mismos, incluso antes de integrarse en aplicaciones espec√≠ficas.

Se consideran GPAI de **riesgo sist√©mico** si:

- Superan un **umbral de c√≥mputo** muy elevado (por ejemplo, m√°s de 10¬≤‚Åµ FLOPs).
- Tienen capacidades de **impacto generalizado** (creaci√≥n de contenido a escala, manipulaci√≥n masiva, vulnerabilidad a abusos).
- Han sido **designados formalmente por la Comisi√≥n Europea** como tales.

Si tu modelo cae en esta categor√≠a, necesitas cumplir con requisitos adicionales:

| **Obligaci√≥n Adicional** | **¬øCumples?** |
| --- | --- |
| Evaluaci√≥n del modelo (auditor√≠as internas, pruebas adversariales) |  |
| Evaluaci√≥n y mitigaci√≥n de riesgos sist√©micos |  |
| Sistema de seguimiento y notificaci√≥n de incidentes graves |  |
| Medidas reforzadas de ciberseguridad |  |

> Estos modelos tambi√©n estar√°n bajo vigilancia activa de la futura **Oficina Europea de IA**, y podr√≠an requerir informes peri√≥dicos, revisi√≥n de pr√°cticas y participaci√≥n en c√≥digos de conducta.
> 

### **¬øY si tu modelo es open source?**

Buena noticia: la Ley **introduce exenciones y ajustes para modelos GPAI de c√≥digo abierto**, siempre que:

- No se usen directamente en sistemas de alto riesgo.
- No representen un riesgo sist√©mico por s√≠ mismos.
- No est√©n integrados en productos comerciales sin garant√≠as.

> Aun as√≠, se recomienda **documentar bien tu modelo y su gobernanza**, para demostrar buena fe y facilitar el cumplimiento si se reutiliza comercialmente.
> 

### **C√≥digos de Pr√°cticas y cumplimiento flexible**

La Oficina de IA de la UE est√° trabajando en **C√≥digos de Pr√°ctica voluntarios**, que servir√°n como gu√≠a para demostrar cumplimiento y buenas pr√°cticas.

Si participas activamente en estos c√≥digos, **podr√°s demostrar cumplimiento proactivo incluso antes de que entren en vigor medidas m√°s estrictas**.

### **¬øPor qu√© es tan importante esta parte?**

Porque por primera vez se reconoce que los **riesgos de la IA no solo vienen del uso final, sino tambi√©n del modelo base**. Esto afecta directamente a:

- Empresas que entrenan y publican modelos fundacionales.
- Plataformas que ofrecen modelos como servicio (MLaaS).
- Equipos que construyen frameworks que luego otros reutilizan.

> Si eres parte del ecosistema t√©cnico de IA (ya sea en producto, ciencia de datos o arquitectura), necesitas revisar tus modelos actuales y futuros con estos criterios.
> 

Si desarrollas o publicas un modelo de prop√≥sito general:

1. **Eval√∫a si cumple con la definici√≥n legal de GPAI.**
2. **Prepara desde ya la documentaci√≥n t√©cnica y la pol√≠tica de derechos.**
3. **Planifica c√≥mo compartir informaci√≥n con tus integradores y usuarios.**
4. **Si tu modelo es muy potente, consulta si puede entrar en riesgo sist√©mico.**
5. **Mantente informado sobre los c√≥digos de pr√°ctica y directrices de la Oficina de IA.**

## **Checklist 6: Gobernanza, formaci√≥n e interacci√≥n con autoridades ‚Äî el marco horizontal que todas las empresas deben aplicar**

---

Hasta ahora hemos hablado de **qu√© hacer seg√∫n el tipo de IA que uses (alto riesgo, limitado, GPAI, etc.)**. Pero la Ley de IA tambi√©n impone una serie de **requisitos transversales u ‚Äúhorizontales‚Äù** que aplican **a todas las empresas involucradas en IA**: tanto proveedores como usuarios, independientemente del nivel de riesgo.

Estos requisitos definen la **gobernanza interna m√≠nima** esperada para operar con IA dentro del marco europeo. Y **algunos de ellos entran en vigor muy pronto**: a partir del **2 de febrero de 2025**.

### **Alfabetizaci√≥n en IA (Art√≠culo 4)**

La ley dice claramente que **toda persona que opere, supervise o tome decisiones con IA debe tener un nivel adecuado de formaci√≥n y comprensi√≥n**.

Esto no se limita a desarrolladores: aplica tambi√©n a:

- Equipos de RR.HH. que usan IA para evaluar candidatos.
- Departamentos legales que supervisan cumplimiento.
- Usuarios finales que activan sistemas autom√°ticos.
- Directivos que deciden sobre la implantaci√≥n de IA.

| **Requisito** | **¬øCumples?** |
| --- | --- |
| ¬øHas iniciado programas de formaci√≥n en IA para tu equipo? |  |
| ¬øIncluyen aspectos t√©cnicos, √©ticos, legales y de negocio? |  |
| ¬øEst√°n adaptados al rol y responsabilidad de cada persona? |  |
| ¬øEst√°s documentando estas acciones formativas? |  |
| ¬øConoces el repositorio oficial de buenas pr√°cticas de la Oficina de IA? |  |

> No esperes a que sea obligatorio. Establecer una cultura de alfabetizaci√≥n en IA ahora no solo cumple con la ley, sino que **te prepara mejor para integrar la IA de forma responsable y eficiente**.
> 

## **Representante autorizado en la UE (Art√≠culo 22) ¬øQui√©n necesita esto?**

---

Si eres un **proveedor de IA fuera de la Uni√≥n Europea**, y tu sistema se usa o se comercializa en Europa, **debes designar un representante autorizado dentro de la UE**.

| **Requisito** | **¬øCumples?** |
| --- | --- |
| ¬øTienes ya un representante legal designado en la UE? |  |
| ¬øEst√° formalmente establecido el mandato de ese representante? |  |
| ¬øIncluye tareas como: cooperaci√≥n con autoridades, mantenimiento de documentaci√≥n, notificaci√≥n de incidentes, etc.? |  |

> Esto es similar a lo que exige el RGPD para los responsables de tratamiento fuera de la UE. Asegura que **exista un punto de contacto legal y operativo dentro de Europa** para cualquier necesidad regulatoria.
> 

### **Interacci√≥n con autoridades competentes**

La Ley de IA te obliga a **cooperar activamente con las autoridades nacionales** y con la futura **Oficina Europea de IA**.

Esto incluye:

- Estar preparado para responder a **requerimientos de informaci√≥n**.
- Tener los documentos listos si te los solicitan (logs, evaluaciones, declaraciones de conformidad, etc.).
- Notificar incidentes relevantes o riesgos detectados.
- Participar, si procede, en **procesos de evaluaci√≥n o investigaci√≥n**.

| **Requisito** | **¬øCumples?** |
| --- | --- |
| ¬øTienes un responsable designado para interlocuci√≥n con reguladores? |  |
| ¬øTienes procesos definidos para responder a solicitudes oficiales? |  |
| ¬øTienes localizados y actualizados los documentos clave de cumplimiento? |  |
| ¬øHas mapeado qu√© autoridad te corresponde seg√∫n tu pa√≠s/sede? |  |

## **Acceso a innovaci√≥n: Sandboxes regulatorios**

La Ley de IA prev√© entornos de prueba controlados (‚Äú**sandboxes**‚Äù) para que las empresas puedan **probar sistemas de IA innovadores sin incurrir en sanciones**, siempre que lo hagan bajo supervisi√≥n.

Esto es especialmente √∫til si est√°s desarrollando un sistema de alto riesgo, una herramienta compleja o un producto con un caso de uso a√∫n no definido del todo.

| **Requisito** | **¬øValorado?** |
| --- | --- |
| ¬øConoces los sandboxes regulatorios disponibles en tu pa√≠s? |  |
| ¬øHas considerado usar uno para validar tus soluciones de IA? |  |
| ¬øTienes un roadmap t√©cnico que podr√≠a beneficiarse de este entorno controlado? |  |

> El uso de sandboxes **no solo acelera el desarrollo legalmente seguro**, tambi√©n puede ayudarte a afinar tu producto con el feedback de reguladores y expertos t√©cnicos.
> 

Los requisitos de este bloque no dependen del tipo de sistema, sino de tu **madurez organizativa frente a la IA**. Implementarlos de forma temprana no solo es cumplimiento normativo: es una se√±al clara de liderazgo responsable en un mercado que se mueve hacia la confianza y la transparencia.

### **5 cosas que puedes hacer ya:**

1. **Dise√±a e implementa un plan de formaci√≥n en IA para todo el personal relevante.**
2. **Revisa si necesitas un representante en la UE y formaliza el acuerdo.**
3. **Asigna a alguien como responsable de relaci√≥n con autoridades.**
4. **Prepara una carpeta centralizada con toda la documentaci√≥n exigida.**
5. **Explora oportunidades de innovaci√≥n bajo sandbox si est√°s desarrollando nuevas soluciones.**

## **Pr√≥ximos pasos: plazos clave y enfoque continuo de cumplimiento**

---

La **Ley de IA de la UE no se aplicar√° de golpe**, sino por fases. Pero no te confundas: esto no significa que puedas esperar al √∫ltimo minuto. Al contrario, **las empresas que empiecen ahora tendr√°n ventajas competitivas claras** y evitar√°n sorpresas desagradables.

Aqu√≠ te dejamos el mapa de ruta que deber√≠as tener en mente: **Tabla resumen: ¬øQu√© entra en vigor y cu√°ndo?**

| **Hito** | **Fecha de aplicaci√≥n** |
| --- | --- |
| **Entrada en vigor de la Ley de IA** | 1 de agosto de 2024 |
| **Prohibiciones y formaci√≥n obligatoria (alfabetizaci√≥n en IA)** | 2 de febrero de 2025 (6 meses) |
| **Normas para modelos de prop√≥sito general (GPAI)** | 2 de agosto de 2025 (12 meses) |
| **Inicio de operaciones de la Oficina de IA y la Junta** | 2 de agosto de 2025 |
| **Aplicaci√≥n total a sistemas de alto riesgo (Anexo III)** | 2 de agosto de 2026 (24 meses) |
| **Aplicaci√≥n a productos regulados con IA (Anexo I)** | 2 de agosto de 2027 (36 meses) |

> Como ves, **las primeras obligaciones comienzan solo seis meses despu√©s de la entrada en vigor**, por lo que el reloj ya est√° corriendo.
> 

### **¬øPor qu√© se habla de cumplimiento continuo?**

Cumplir con la Ley de IA **no es algo que haces una vez y ya est√°**. No basta con auditar o documentar tu sistema al inicio. Necesitar√°s implementar una **cultura de cumplimiento viva y mantenida en el tiempo**. Esto implica:

- **Actualizar tus evaluaciones peri√≥dicamente**: Gesti√≥n de riesgos, FRIA, documentaci√≥n t√©cnica, monitoreo.
- **Reaccionar a cualquier modificaci√≥n sustancial** en tus sistemas, modelos o flujos de uso.
- **Estar siempre al d√≠a** de:
    - Nuevas normas armonizadas.
    - Directrices emitidas por la Comisi√≥n Europea o la Oficina de IA.
    - Cambios en anexos o nuevos actos delegados.

La propia Ley prev√© que estos elementos puedan **evolucionar din√°micamente**, por lo que te exige una **estructura de gobernanza que no sea est√°tica**.

## **Recordatorio: las sanciones pueden ser muy elevadas**

---

El incumplimiento de la Ley de IA **puede salir muy caro**. Las multas m√°ximas previstas son hasta **35 millones de euros**, o Hasta el **7‚ÄØ% del volumen de negocio global anual**, (la cifra que sea mayor).

> Estas sanciones son comparables (o incluso superiores) a las del RGPD. No son te√≥ricas: si usas IA sin cumplir, **est√°s asumiendo un riesgo legal, financiero y reputacional muy alto.**
> 

## **Recomendaciones clave para actuar ahora**

---

No dejes esto en manos de un solo equipo. Implica a las √°reas clave de tu organizaci√≥n:

1. **Dise√±a un plan de cumplimiento progresivo**, alineado con los hitos regulatorios.
2. **Asigna responsables por √°reas**: legal, compliance, tecnolog√≠a, producto, seguridad.
3. **Crea un inventario actualizado** de todos los sistemas de IA que uses, desarrolles o adquieras.
4. **Eval√∫a el nivel de riesgo de cada uso y modelo**.
5. **Revisa tu cadena de proveedores**: ¬øtienen sus propias obligaciones? ¬øte est√°n compartiendo la informaci√≥n necesaria?
6. **Documenta cada paso**: decisiones, evaluaciones, medidas adoptadas.

Y sobre todo:

- **Busca asesoramiento t√©cnico y legal especializado**.
- **Participa en foros, sandboxes y redes de cumplimiento**: te dar√°n acceso anticipado a interpretaciones pr√°cticas y gu√≠as sectoriales.
- **Piensa en el cumplimiento como un diferenciador competitivo**, no como una carga administrativa.

## **Conclusi√≥n: construir una IA fiable requiere estrategia, no reacci√≥n**

---

La Ley de IA marca un antes y un despu√©s. Si solo piensas en ‚Äúcumplir para no ser multado‚Äù, est√°s perdiendo la oportunidad de posicionarte como una empresa madura, transparente y confiable en el uso de tecnolog√≠a avanzada.

Cumplir con esta ley no es solo evitar sanciones. Es **crear un marco √©tico, legal y operativo que inspire confianza**: en tus clientes, tus empleados, tus inversores y en la sociedad.
